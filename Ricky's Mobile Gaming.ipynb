{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Ricky's Mobile Gaming with Spark\n",
    "################################\n",
    "This script uses a demo data set from Mixpanel for a hypothetical mobile game.  Our goal here is to \n",
    "build a random forest model to predict which users are most likely to make a purchase with the game.\n",
    "This script will demonstrate a few key features of Apache Spark as we transform the data, train our\n",
    "model, and make predictions on the data set.  This is not meant to be a definitive example of \n",
    "implementing machine learning, but is meant to demonstrate using Apache Spark for an ML application.\n",
    "\n",
    "Some notes on what I've done:  I wanted to demonstrate a few key features of Spark:\n",
    "1. Using mapping a la MapReduce to transform my data\n",
    "2. Demonstrating the use of Spark Dataframes\n",
    "3. Using accumulators to act as counters\n",
    "\n",
    "I trade off in my solution between these three features to show how to use each of them.  Enjoy!!\n",
    "################################\n",
    "'''\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Initialize constants\n",
    "EVENT_INDEX = 0\n",
    "DISTINCT_ID_INDEX = 19\n",
    "EVENT_LIST = ['Level Completed', 'In-App Purchase', 'Game Played', 'Tutorial Exited', 'App Install', 'Character Created', '$experiment_started', 'Registration Complete', '$campaign_received', 'App Open', 'Session End', '$campaign_delivery']\n",
    "TARGET_EVENT = 'In-App Purchase'\n",
    "\n",
    "# Initialize accumulators for aggregate counting\n",
    "count = sc.accumulator(0)\n",
    "numCorr = sc.accumulator(0)\n",
    "numFP = sc.accumulator(0)\n",
    "numFN = sc.accumulator(0)\n",
    "\n",
    "# Custom partitioner that splits data based on distinct_id associated with each event\n",
    "def getDistinctId(row):\n",
    "    r = []\n",
    "    r = row.split(\",\")\n",
    "    return r[DISTINCT_ID_INDEX]\n",
    "\n",
    "# Map function that takes each row of data and outputs (distinct_id, event_name)\n",
    "def idEventMapper(x):\n",
    "    row = x.split(\",\")\n",
    "    id = row[DISTINCT_ID_INDEX].encode('ascii', 'ignore')\n",
    "    event = row[EVENT_INDEX].encode('ascii', 'ignore')\n",
    "    return [id, event]\n",
    "\n",
    "# Function that takes events grouped by distinct_id and outputs (id, freqDict)\n",
    "# where freqDict is a dictionary with key=event_name, value=num times user has done event\n",
    "def freqDictBuilder(x):\n",
    "    id = x[0]\n",
    "    freqDict = {}\n",
    "    for each in x[1]:\n",
    "        if (each in freqDict):\n",
    "            freqDict[each] += 1\n",
    "        else:\n",
    "            freqDict[each] = 1\n",
    "\n",
    "    return [id, freqDict]\n",
    "\n",
    "# Function that takes EVENT_LIST and removes TARGET_EVENT\n",
    "def makeEventList(target):\n",
    "    eventVec = []\n",
    "    for each in EVENT_LIST:\n",
    "        if (each != target):\n",
    "            eventVec.append(each)\n",
    "    return eventVec\n",
    "\n",
    "# Map function to convert FreqDict RDD to LabeledPoint RDD\n",
    "def makeLabelPoints(row, eventList, target):\n",
    "    vals = row[1]\n",
    "    l = 0\n",
    "    keys = vals.keys()\n",
    "    if (target in keys):\n",
    "        l = 1\n",
    "    \n",
    "    vec = []\n",
    "    for each in eventList:\n",
    "        if (each in keys):\n",
    "            ct = vals[each]\n",
    "        else:\n",
    "            ct = 0\n",
    "        vec.append(ct)\n",
    "\n",
    "    count.add(1)\n",
    "    return LabeledPoint(l, vec)\n",
    "\n",
    "# Helper functions for counting number of correct predictions, false positives, and false negatives\n",
    "def getCorrAndFP(row):\n",
    "    actual = int(row['label'])\n",
    "    prediction = int(row['prediction'])\n",
    "    numCorrect(actual, prediction)\n",
    "    numFalsePos(actual, prediction)\n",
    "\n",
    "def numCorrect(actual, prediction):\n",
    "    if (actual == prediction):\n",
    "        numCorr.add(1)\n",
    "\n",
    "def numFalsePos(actual, prediction):\n",
    "    if (actual == 0 & prediction == 1):\n",
    "        numFP.add(1)\n",
    "\n",
    "def numFalseNeg(actual, prediction):\n",
    "    if (prediction == 0 & actual == 1):\n",
    "        numFN.add(1)\n",
    "\n",
    "# Main Function - load data and run mapper to extract tuple of (distinct_id, event_name)\n",
    "r = sc.textFile(\"rickys_data.csv\", 200, getDistinctId)\n",
    "result = r.map(lambda x: idEventMapper(x)).filter(lambda x: x[0] != '').filter(lambda x: x[0] != 'property_distinct_id')\n",
    "\n",
    "# Group tuples by distinct_id, then build Frequency Dictionary for each distinct_id\n",
    "res = result.groupByKey()\n",
    "f = res.map(lambda x: freqDictBuilder(x))\n",
    "\n",
    "# Take list of events and remove our target event (one we are predicting)\n",
    "eventL = makeEventList(TARGET_EVENT)\n",
    "\n",
    "# Build RDD of LabeledPoints to train Random Forest: LabeledPoint(classification, feature vector)\n",
    "labels = f.map(lambda x: makeLabelPoints(x, eventL, TARGET_EVENT))\n",
    "\n",
    "# train model on RDD of LabeledPoints - see https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.tree\n",
    "model = RandomForest.trainClassifier(labels, 2, {}, 4, seed=42)\n",
    "\n",
    "# make predictions\n",
    "print model.predict([9.0,9.0,1.0,1.0,7.0,1.0,1.0,0.0,2.0,1.0,1.0])\n",
    "print model.predict([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0])\n",
    "print model.predict([3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0])\n",
    "\n",
    "# Demonstrating Spark DataFrames here by creating a DataFrame and making predictions over each user and adding new column\n",
    "labels_df = SQLContext(sc).createDataFrame(labels)\n",
    "predictions = model.predict(labels_df.map(lambda row: row.features))\n",
    "predicted_df = labels_df.map(lambda row: row.label).zip(predictions).toDF().withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'prediction')\n",
    "\n",
    "# Variables for the number of predicted and actual positives and negatives\n",
    "predNeg = predicted_df[predicted_df['prediction'] == 0].count()\n",
    "predPos = predicted_df[predicted_df['prediction'] == 1].count()\n",
    "actNeg = predicted_df[predicted_df['label'] == 0].count()\n",
    "actPos = predicted_df[predicted_df['label'] == 1].count()\n",
    "\n",
    "# Applying my wrapper function to get number of correct predictions, false positives, and fales negatives.\n",
    "# This is an example of a row-wise operation with Pandas and Spark\n",
    "predicted_df.map(lambda row: getCorrAndFP(row))\n",
    "numCorrect = numCorr.value\n",
    "numFalsePos = numFP.value\n",
    "numFalseNeg = numFN.value\n",
    "\n",
    "# Calculation of percent of predictions that are correct, precision, and recall\n",
    "ct = predicted_df.count()\n",
    "pctCorr = numCorrect / float(ct)\n",
    "precision = float(actPos) / (actPos + numFalsePos)\n",
    "recall = float(actPos) / (actPos + numFalseNeg)\n",
    "\n",
    "# Output results\n",
    "print \"Total percent correct: %.2f\" % pctCorr\n",
    "print \"Precision: %.2f\" % precision\n",
    "print \"Recall: %.2f\" % recall\n",
    "print \"F1 Score: %.2f\" % float(2 * (precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
