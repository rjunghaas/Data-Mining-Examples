Data Mining Examples README

This is some sample code from assignments in a Data Mining course from UC Berkeley's
School of Information (INFO 290T).  Additional scripts have been added to demonstrate Naive Bayes, K-Means Clustering, Decision Trees, and Logistic Regression. The code presented here is for:

1. Back Propagation.  This is the algorithm for constructing a neural network.  The
algorithm will converge upon the optimal weights for each node which can then offer the
best predicted output value.  This is an example of basic machine learning.

2. Naive Bayes. This is a generalized script that takes two features and uses Bayes's Theorem to make predictions.  The user will need to adapt this code to the data set with which they are working.

3. Clustering. A simple demonstration of K-Means clustering across multiple features.  This algorithm uses mean-centering to normalize multiple features and then calculates distance by L2-Norm.

4. Decision Tree. Simple demonstration of a recursive decision tree algorithm on UCI's mushroom data set.

5. Logistic Regression.  Demonstration of a logistic regression algorithm using Newton's Method to converge on parameters.
